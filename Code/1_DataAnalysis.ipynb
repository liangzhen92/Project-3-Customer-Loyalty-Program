{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are provided in Kaggle. There are two main datasets: historical transactions data record all historical transaction information (transaction date, purchased products, purchase amount, etc.) and new_merchant_transactions (same features as historical transactions). \n",
    "\n",
    "The greatest challenge of this project is that the dataset is large with 2M observations. Finding the best algorithm which works on large dataset is also a challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# merge two dataset into one\n",
    "df = pd.read_csv('historical_transactions.csv')\n",
    "df_new_metchants = pd.read_csv('new_merchant_transactions.csv')\n",
    "df_combined = pd.concat([df, df_new_metchants])\n",
    "df_combined.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_train = pd.read_csv('train.csv')\n",
    "# The datasets are simulated and not real, so the data needs cleaning\n",
    "df_train=df_train.drop(df_train.index[df_train['target'] <= -10])\n",
    "df_train=df_train.drop(df_train.index[df_train['target'] >= 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# bin the continuous target variable to three categories\n",
    "df_train.drop(df_train.index[df_train['target'] <= -10])\n",
    "bins=[-10, -1, 1, 10]\n",
    "df_train['target_binned'] = pd.cut(df_train['target'], bins)\n",
    "df_train['target_binned'] = df_train['target_binned'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_target (row):\n",
    "    if row['target_binned'] == '(-1, 1]':\n",
    "        return \"0\"\n",
    "    if row['target_binned'] == '(-10, -1]':\n",
    "        return \"-1\"\n",
    "    if row['target_binned'] == '(1, 10]':\n",
    "        return \"1\"\n",
    "    return np.nan\n",
    "  \n",
    "df_train['target_binned'] = df_train.apply (lambda row: bin_target (row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(subset=['target_binned'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['target_binned'] == \"1\"].count()['target_binned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep original features\n",
    "df_train['org_feature_1'] = df_train['feature_1']\n",
    "df_train['org_feature_2'] = df_train['feature_2']\n",
    "df_train['org_feature_3'] = df_train['feature_3']\n",
    "        \n",
    "def convert_type(df):\n",
    "    column_list = ['feature_1','feature_2','feature_3']\n",
    "    for i in column_list:\n",
    "        df[i] = df[i].astype(str)\n",
    "        \n",
    "# df_train.apply (lambda row: convert_type(row), axis=1)\n",
    "\n",
    "convert_type(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.merge(df_combined,\n",
    "                  df_train,\n",
    "                  on='card_id',\n",
    "                  how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep original features\n",
    "df_target['org_authorized_flag'] = df_target['authorized_flag']\n",
    "df_target['org_category_1'] = df_target['category_1']\n",
    "df_target['org_category_3'] = df_target['category_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['purchase_date'] = pd.to_datetime(df_target['purchase_date'])\n",
    "df_target_2018 = df_target[:]\n",
    "df_target_2018 = df_target_2018[df_target_2018['purchase_date'].dt.year >= 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting raw data into training/testing data by card_id not row index\n",
    "# Because the later will seperate multiple transactions records from one card holder to different training/testing sample.\n",
    "import numpy as np\n",
    "\n",
    "def get_user_split_data(df, test_size=.2, seed=42):\n",
    "\n",
    "    rs = np.random.RandomState(seed)\n",
    "    \n",
    "    total_users = df['card_id'].unique() \n",
    "    test_users = rs.choice(total_users, \n",
    "                           size=int(total_users.shape[0] * test_size), \n",
    "                           replace=False)\n",
    "\n",
    "    df_tr = df[~df['card_id'].isin(test_users)]\n",
    "    df_te = df[df['card_id'].isin(test_users)] \n",
    "\n",
    "    y_tr, y_te = df_tr['target_binned'], df_te['target_binned']\n",
    "    X_tr = df_tr[['installments','purchase_amount','category_2','feature_1_2','feature_1_3','feature_1_4','feature_1_5','feature_2_2', 'feature_2_3', 'feature_3_1','auth_flag_Y','category_3_B','category_3_C','category_1_N']] \n",
    "    X_te = df_te[['installments','purchase_amount','category_2','feature_1_2','feature_1_3','feature_1_4','feature_1_5','feature_2_2', 'feature_2_3', 'feature_3_1','auth_flag_Y','category_3_B','category_3_C','category_1_N']]\n",
    "\n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = get_user_split_data(df_target_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is imbalance?**\n",
    "Imbalance means rare events data, binary dependent variables with dozens to thousands of times fewer ones (events, such as wars, vetoes, cases of political activism, or epidemiological infections) than zeros (“nonevents”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the impacts if the target variable is imbalanced?** \n",
    "Assume if you are trying to predict fraud or cancer, and the occurance of fraud/cancer is vary rare (2% of the population). If model does not predict anything by just assigning all prediction to non-fraud/healthy, the accuracy of the model still very high (98%!). But do you consider model performance acceptable? The recall is actually very low (0%). \n",
    "\n",
    "Statistically, popular statistical procedures, such as logistic regression, can sharply underestimate the probability of rare events. Why? Linear regression models are invariant to the (unconditional) mean of the dependent variables. However, the same is not true for binary dependent variable models. The mean of a binary variable is the relative frequency of events in the data, which, in addition to the number of observations, constitutes the information content of the data set. \n",
    "By studying the variance matrix:\n",
    "$$V(\\hat{\\beta}) = [\\sum_{i=1}^{n}\\pi_{i}(1-\\pi_{i})X'_{i}X_{i}] ^{-1}$$\n",
    "The part of this matrix affected by rare events is the factor $\\pi_{i}(1-\\pi_{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the typical approach to handle imbalance?**\n",
    "\n",
    "_Oversampling_\n",
    "\n",
    "RandomOverSampler to handle imbalanced target variable: does what its name implies. Takes the minority class and over-samples it until it is balanced with the majority class.\n",
    "\n",
    "_Generate Synthetic data_\n",
    "\n",
    "1) SMOTE - Synthetic Minority Oversampling Technique: start with a point in the monority class, choose one of the k nearest neighbors, add a new point between them. Two main approaches: SMOTE and ADASYN. (**Ans:** SMOTE does not differentiate between points near the decision boundary and points far away from it. Thus, it generated new points in areas that did not matter for the classifier.)\n",
    "\n",
    "2) ADASYN - ADAptive SYNthetic oversampling. Instead of generating synthetic observations between any minority points, it puts more emphasis on the regions where the class imbalance is greatest. In other words, in the regions where the classifier is most likely to predict the majority class.\n",
    "\n",
    "Which oversampling method worked best for **this** dataset? Why? \n",
    "\n",
    "**Ans:** This depends. Understand the business reason and the costs associated with false positives/negatives to determine what is an acceptable trade-off.\n",
    "\n",
    "_Undersampling_\n",
    "\n",
    "Undersampling is the opposite of Oversampling. It takes the majority class and under-samples it until it is balanced with the minority class.\n",
    "\n",
    "**In what scenarios would this method be useful?**\n",
    "\n",
    "**Ans:** If your model is computationally expensive and doubling the size of the data would hurt performance, undersampling would be a better approach.\n",
    "\n",
    "Fortunately, the methods we covered for binary classification still work in a multiclass setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Are there any different approaches?** Yes.\n",
    "\n",
    "The two corrections are: 1) Prior correction: Prior correction involves computing the usual logistic regression MLE and correcting the estimates based on prior information about the fraction of ones in the population, τ, and the observed fraction of ones in the sample (or sampling probability), $\\bar{y}$. For the logit model, in any of the above sampling designs, the MLE $\\hat{\\beta}_{1}$ is a statistically consistent estimate of β1 and the following corrected estimate is consistent for $\\beta_{0}$: \n",
    "$$\\beta_{0}-ln[(\\frac{1-τ}{τ})(\\frac{\\bar{y}}{1-\\bar{y}})]$$\n",
    "\n",
    "2) Weighting: An alternative procedure is to weight the data to compensate for differences in the sample $\\bar{y}$ and population τ fractions of ones induced by choice-based sampling. The weighted log-likelihood: \n",
    "$$lnL_{w}(\\beta|y) = w_{1}\\sum_{Y_{i}=1}ln(\\pi_{i})+w_{0}\\sum_{Y_{i}=0}ln(1-\\pi_{i})$$\n",
    "where $w_{1} = τ/\\bar{y}$ and $w_{0} = (1 - τ)/(1 - \\bar{y})$.\n",
    "\n",
    "All information is coming from this paper, for more information, read here: [Logistic Regression in Rare Events Data](https://gking.harvard.edu/files/0s.pdf)\n",
    "\n",
    "\n",
    "**How to determine if the target variable is imbalanced or not?**\n",
    "If there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap ###\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- Which method we use will be dependent on the problem. For example, sometimes ADAYSN will work great, in other cases, not so much. \n",
    "- If the dataset is small to begin with, undersampling may reduce your data too much and many classifiers will have difficulty generalizing.\n",
    "- Oversampling methods may prove to be computationally intensive depending which algorithm is being used. Find out what limitations you have an adjust accordingly.\n",
    "- Always think of the business case! Is misclassifying the majority class just as bad as misclassifying the minority class? What is the right metric for your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling for imbalanced data\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_resampled)\n",
    "df.head()\n",
    "# df['target_binned'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_sample(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix  \n",
    "  \n",
    "# training a DescisionTreeClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree_model = DecisionTreeClassifier(max_depth = 20).fit(X_resampled, y_resampled) \n",
    "dtree_predictions = dtree_model.predict(X_te) \n",
    "# accuracy = knn.score(X_te, y_te) \n",
    "\n",
    "# creating a confusion matrix \n",
    "cm = confusion_matrix(y_te, dtree_predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=18):\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names, )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label', fontsize=20)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y_te, dtree_predictions), ['Class Bad ', 'Class Neutral', 'Class Good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['purchase_date'] = pd.to_datetime(df_target['purchase_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate the transaction data and make it rolling sum purchase amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['rolling_sum'] = df_target.groupby(['card_id'])['purchase_amount'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['purchase_date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['date_month'] = df_target['purchase_date'].dt.day\n",
    "df_target['abs_purchase_amount'] = abs(df_target['purchase_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_month_1 = df_target.groupby(['card_id','date_month'],as_index=False).agg({'purchase_amount':['sum','mean','count']})\n",
    "date_month_1.columns = ['_'.join(col).strip() for col in date_month_1.columns.values]\n",
    "date_month_1.columns = ['card_id', 'date_month', 'purchase_amount_month_sum', 'purchase_amount_month_mean', 'purchase_amount_month_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(date_month_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.merge(df_target, date_month_1, on=['card_id','date_month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.lineplot(x=\"date_month\", y=\"abs_purchase_amount\", data=date_month_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if the pattern still hold when I split the data by year and do the same analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_2017 = df_target[df_target['purchase_date'].dt.year == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_month_2017 = df_target_2017.groupby(['date_month'],as_index=False).agg({'abs_purchase_amount':'sum'})\n",
    "date_month_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.lineplot(x=\"date_month\", y=\"abs_purchase_amount\", data=date_month_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_2018 = df_target[df_target['purchase_date'].dt.year == 2018]\n",
    "date_month_2018 = df_target_2018.groupby(['date_month'],as_index=False).agg({'purchase_amount':'sum'})\n",
    "date_month_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.lineplot(x=\"date_month\", y=\"purchase_amount\", data=date_month_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['first_active_month'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Remove date around 12-20 to 12-30 to reduce noise\n",
    "import datetime\n",
    "\n",
    "df_wo_noise = df_target[(df_target['purchase_date'].dt.date < datetime.date(2018, 12, 23)) | (df_target['purchase_date'].dt.date > datetime.date(2018, 12, 29))]\n",
    "df_wo_noise = df_wo_noise[(df_wo_noise['purchase_date'].dt.date < datetime.date(2017, 12, 23)) | (df_wo_noise['purchase_date'].dt.date > datetime.date(2017, 12, 29))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_2017 = df_wo_noise[df_wo_noise['purchase_date'].dt.year == 2017 ]\n",
    "date_month_2017 = df_target_2017.groupby(['date_month'],as_index=False).agg({'abs_purchase_amount':'sum'})\n",
    "ax = sns.lineplot(x=\"date_month\", y=\"abs_purchase_amount\", data=date_month_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_2018 = df_wo_noise[df_wo_noise['purchase_date'].dt.year == 2018]\n",
    "date_month_2018 = df_target_2018.groupby(['date_month'],as_index=False).agg({'abs_purchase_amount':'sum'})\n",
    "ax = sns.lineplot(x=\"date_month\", y=\"abs_purchase_amount\", data=date_month_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_month = df_wo_noise.groupby(['date_month'],as_index=False).agg({'abs_purchase_amount':'sum'})\n",
    "ax = sns.lineplot(x=\"date_month\", y=\"abs_purchase_amount\", data=date_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_investigation = df_wo_noise[df_wo_noise['date_month'] == 23]\n",
    "issue_investigation['purchase_date'] = issue_investigation['purchase_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_investigation_1 = issue_investigation.groupby(['purchase_date'],as_index=False).agg({'abs_purchase_amount':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_investigation_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_wo_noise = df_wo_noise[(df_wo_noise['purchase_date'].dt.date != datetime.date(2017, 4, 23))]\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_wo_noise_1 = df_target[(df_target['card_id'] != 'C_ID_3b6ac8e52d') | (df_target['authorized_flag'] != 'N')]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_month = df_wo_noise_1.groupby(['date_month'],as_index=False).agg({'abs_purchase_amount':'mean'})\n",
    "ax = sns.lineplot(x=\"date_month\", y=\"abs_purchase_amount\", data=date_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_target[(df_target['purchase_date'].dt.date == datetime.date(2017, 4, 23))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby(['card_id'],as_index=False).agg({'abs_purchase_amount':'sum'}).sort_values(['abs_purchase_amount'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target[df_target['card_id'] == 'C_ID_3b6ac8e52d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze day of week pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['day_of_week'] = df_target['purchase_date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.head()\n",
    "len(df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofweek = df_target.groupby(['card_id','day_of_week'],as_index=False).agg({'purchase_amount':['sum','mean','count']})\n",
    "dayofweek.columns = ['_'.join(col).strip() for col in dayofweek.columns.values]\n",
    "dayofweek.columns = ['card_id', 'day_of_week', 'purchase_amount_dayofweek_sum', 'purchase_amount_dayofweek_mean', 'purchase_amount_dayofweek_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.merge(df_target, dayofweek, on=['card_id','day_of_week'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze pattern by Hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['hour'] = df_target['purchase_date'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['hour'] = df_target['hour'].apply(lambda x: x.strftime('%H-%M-%S'))\n",
    "df_target['hour'] = df_target['hour'].apply(lambda x: x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_hour = df_target.groupby(['card_id','hour'],as_index=False).agg({'purchase_amount':['sum','mean','count']})\n",
    "date_hour.columns = ['_'.join(col).strip() for col in date_hour.columns.values]\n",
    "date_hour.columns = ['card_id', 'hour', 'purchase_amount_hour_sum', 'purchase_amount_hour_mean', 'purchase_amount_hour_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.merge(df_target, date_hour, on=['card_id','hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourofday = df_target.groupby(['hour'],as_index=False).agg({'abs_purchase_amount':'count'})\n",
    "ax = sns.lineplot(x=\"hour\", y=\"abs_purchase_amount\", data=hourofday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exam people's purchase behavior across different state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bystate = df_target.groupby(['state_id'],as_index=False).agg({'abs_purchase_amount':'mean'})\n",
    "ax = sns.lineplot(x='state_id', y=\"abs_purchase_amount\", data=bystate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bycity = df_target.groupby(['city_id'],as_index=False).agg({'abs_purchase_amount':'count'})\n",
    "ax = sns.lineplot(x='city_id', y=\"abs_purchase_amount\", data=bycity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.groupby(['merchant_category_id'],as_index=False).agg({'abs_purchase_amount':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze Usage Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df_target['first_active_month'] = pd.to_datetime(df_target['first_active_month'])\n",
    "        \n",
    "df_target['month_diff'] = ((datetime.datetime.today() - df_target['purchase_date']).dt.days)//30                                        \n",
    "df_target['month_diff'] += df_target['month_lag']\n",
    "df_target['elapsed_time'] = (datetime.date(2018, 2, 1) - df_target['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "purchase_month = df_target.groupby(['card_id','elapsed_time'],as_index=False).agg(OrderedDict([('purchase_amount','count')]))\n",
    "purchase_month.columns = ['card_id', 'elapsed_time','# of purchase']\n",
    "\n",
    "purchase_month['order_freq'] = purchase_month['# of purchase'] / purchase_month['elapsed_time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_month = purchase_month[['card_id', 'order_freq']]\n",
    "purchase_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.merge(df_target, purchase_month, on=['card_id'], how='left')\n",
    "len(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_amount = df_target.groupby(['card_id'],as_index=False).agg(OrderedDict([('purchase_amount','sum'),('purchase_date','count')]))                  \n",
    "customer_amount.columns = ['card_id', 'purchase_amount', 'total_usage']\n",
    "customer_amount['amount_per_use'] = customer_amount['purchase_amount'] / customer_amount['total_usage']\n",
    "customer_amount = customer_amount[['card_id','total_usage','amount_per_use']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.merge(df_target, customer_amount, on=['card_id'], how='left')\n",
    "len(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('my_dataframe_newfeatures.pickle','rb') as read_file:\n",
    "    df_target = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_target.to_csv('my_dataframe_newfeatures.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_target['target_binned']\n",
    "X = df_target[['installments','merchant_category_id','purchase_amount','month_lag','category_2','rolling_sum','date_month','abs_purchase_amount', 'day_of_week','hour','purchase_amount_month_sum',\n",
    "              'purchase_amount_month_mean','purchase_amount_month_count', 'purchase_amount_dayofweek_sum', 'purchase_amount_dayofweek_mean','purchase_amount_dayofweek_count',\n",
    "              'purchase_amount_hour_sum','purchase_amount_hour_mean','purchase_amount_hour_count', 'auth_flag_Y','category_1_N', 'category_3_B','category_3_C','feature_1_2','feature_1_3','feature_1_4','feature_2_3','feature_3_1','feature_1_5','feature_2_2',\n",
    "              'feature_1_2','feature_1_3','feature_1_4','feature_1_5','feature_2_2', 'feature_2_3', 'feature_3_1','auth_flag_Y','category_3_B','category_3_C','category_1_N', 'month_diff', 'elapsed_time', 'order_freq', 'total_usage', 'amount_per_use']] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.get_dummies(df_target, prefix=['auth_flag'], columns=['authorized_flag'], drop_first=True)\n",
    "df_target = pd.get_dummies(df_target, prefix=['category_1'], columns=['category_1'])\n",
    "df_target = pd.get_dummies(df_target, prefix=['category_3'], columns=['category_3'],  drop_first=True)\n",
    "df_target = pd.get_dummies(df_target, prefix=['feature_1'], columns=['feature_1'], drop_first=True)\n",
    "df_target = pd.get_dummies(df_target, prefix=['feature_2'], columns=['feature_2'], drop_first=True)\n",
    "df_target = pd.get_dummies(df_target, prefix=['feature_3'], columns=['feature_3'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['auth_flag_Y'] = df_target['auth_flag_Y'].astype(int)\n",
    "df_target['category_1_N'] = df_target['category_1_N'].astype(int)\n",
    "df_target['category_3_B'] = df_target['category_3_B'].astype(int)\n",
    "df_target['category_3_C'] = df_target['category_3_C'].astype(int)\n",
    "df_target['feature_1_5'] = df_target['feature_1_5'].astype(int)\n",
    "df_target['feature_2_2'] = df_target['feature_2_2'].astype(int)\n",
    "\n",
    "df_target.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['feature_1_2'] = df_target['feature_1_2'].astype(int)\n",
    "df_target['feature_1_3'] = df_target['feature_1_3'].astype(int)\n",
    "df_target['feature_1_4'] = df_target['feature_1_4'].astype(int)\n",
    "df_target['feature_2_3'] = df_target['feature_2_3'].astype(int)\n",
    "df_target['feature_3_1'] = df_target['feature_3_1'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a random forest model and compute the feature importances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomforest = RandomForestClassifier().fit(X, y) \n",
    "\n",
    "# forest = ExtraTreesClassifier(n_estimators=250,\n",
    "#                               random_state=0)\n",
    "\n",
    "# forest.fit(np.array(X), y)\n",
    "importances = randomforest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in randomforest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame()\n",
    "features = ['installments','merchant_category_id','purchase_amount','month_lag','category_2','rolling_sum','date_month','abs_purchase_amount', 'day_of_week','hour','purchase_amount_month_sum',\n",
    "              'purchase_amount_month_mean','purchase_amount_month_count', 'purchase_amount_dayofweek_sum', 'purchase_amount_dayofweek_mean','purchase_amount_dayofweek_count',\n",
    "              'purchase_amount_hour_sum','purchase_amount_hour_mean','purchase_amount_hour_count', 'auth_flag_Y','category_1_N', 'category_3_B','category_3_C','feature_1_2','feature_1_3','feature_1_4','feature_2_3','feature_3_1','feature_1_5','feature_2_2',\n",
    "              'feature_1_2','feature_1_3','feature_1_4','feature_1_5','feature_2_2', 'feature_2_3', 'feature_3_1','auth_flag_Y','category_3_B','category_3_C','category_1_N','month_diff', 'elapsed_time', 'order_freq', 'total_usage', 'amount_per_use']\n",
    "feature_importance_df['feature'] = features\n",
    "feature_importance_df['importance'] = randomforest.feature_importances_\n",
    "# feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feature_importance_df.to_csv('result_analysis_1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = print_confusion_matrix(confusion_matrix(y,randomforest.predict(X)), ['Class Bad ', 'Class Neutral', 'Class Good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='timepoint', y=\"signal\", hue=\"event\", data=df_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
